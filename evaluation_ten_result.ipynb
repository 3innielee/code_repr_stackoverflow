{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "[Curated Stack Overflow pair of intent and code snippet](https://conala-corpus.github.io/)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import nltk\n",
    "import traceback\n",
    "import astor\n",
    "import token as tk\n",
    "from tokenize import generate_tokens\n",
    "from io import StringIO\n",
    "import itertools \n",
    "from gensim.models import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "from time import time\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "train_path=\"data/conala-corpus/conala-train.json\"\n",
    "test_path=\"data/conala-corpus/conala-test.json\"\n",
    "train_clean_output_path=\"data/conala-corpus/.train.seq2seq\"\n",
    "test_clean_output_path=\"data/conala-corpus/.test.seq2seq\"\n",
    "wordembedding_file_path=\"data/conala-corpus/embeddings.txt\"\n",
    "docembedding_file_path=\"data/document_embeddings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUOTED_STRING_RE = re.compile(r\"(?P<quote>[`'\\\"])(?P<string>.*?)(?P=quote)\")\n",
    "\n",
    "\n",
    "def canonicalize_intent(intent):\n",
    "    str_matches = QUOTED_STRING_RE.findall(intent)\n",
    "\n",
    "    slot_map = dict()\n",
    "\n",
    "    return intent, slot_map\n",
    "\n",
    "\n",
    "def replace_strings_in_ast(py_ast, string2slot):\n",
    "    for node in ast.walk(py_ast):\n",
    "        for k, v in list(vars(node).items()):\n",
    "            if k in ('lineno', 'col_offset', 'ctx'):\n",
    "                continue\n",
    "            # Python 3\n",
    "            # if isinstance(v, str) or isinstance(v, unicode):\n",
    "            if isinstance(v, str):\n",
    "                if v in string2slot:\n",
    "                    val = string2slot[v]\n",
    "                    # Python 3\n",
    "                    # if isinstance(val, unicode):\n",
    "                    #     try: val = val.encode('ascii')\n",
    "                    #     except: pass\n",
    "                    setattr(node, k, val)\n",
    "                else:\n",
    "                    # Python 3\n",
    "                    # if isinstance(v, str):\n",
    "                    #     str_key = unicode(v)\n",
    "                    # else:\n",
    "                    #     str_key = v.encode('utf-8')\n",
    "                    str_key = v\n",
    "\n",
    "                    if str_key in string2slot:\n",
    "                        val = string2slot[str_key]\n",
    "                        if isinstance(val, str):\n",
    "                            try: val = val.encode('ascii')\n",
    "                            except: pass\n",
    "                        setattr(node, k, val)\n",
    "\n",
    "\n",
    "def canonicalize_code(code, slot_map):\n",
    "    string2slot = {x[1]['value']: x[0] for x in list(slot_map.items())}\n",
    "\n",
    "    py_ast = ast.parse(code)\n",
    "    replace_strings_in_ast(py_ast, string2slot)\n",
    "    canonical_code = astor.to_source(py_ast)\n",
    "\n",
    "    return canonical_code\n",
    "\n",
    "\n",
    "def decanonicalize_code(code, slot_map):\n",
    "    try:\n",
    "        slot2string = {x[0]: x[1]['value'] for x in list(slot_map.items())}\n",
    "        py_ast = ast.parse(code)\n",
    "        replace_strings_in_ast(py_ast, slot2string)\n",
    "        raw_code = astor.to_source(py_ast)\n",
    "      # for slot_name, slot_info in slot_map.items():\n",
    "      #     raw_code = raw_code.replace(slot_name, slot_info['value'])\n",
    "\n",
    "        return raw_code.strip()\n",
    "    except:\n",
    "        return code\n",
    "\n",
    "def detokenize_code(code_tokens):\n",
    "    newline_pos = [i for i, x in enumerate(code_tokens) if x == '\\n']\n",
    "    newline_pos.append(len(code_tokens))\n",
    "    start = 0\n",
    "    lines = []\n",
    "    for i in newline_pos:\n",
    "        line = ' '.join(code_tokens[start: i])\n",
    "        start = i + 1\n",
    "        lines.append(line)\n",
    "\n",
    "    code = '\\n'.join(lines).strip()\n",
    "\n",
    "    return code\n",
    "\n",
    "\n",
    "def encode_tokenized_code(code_tokens):\n",
    "    tokens = []\n",
    "    for token in code_tokens:\n",
    "        if token == '\\t':\n",
    "            tokens.append('_TAB_')\n",
    "        elif token == '\\n':\n",
    "            tokens.append('_NEWLINE_')\n",
    "\n",
    "\n",
    "def get_encoded_code_tokens(code):\n",
    "    code = code.strip()\n",
    "    #print(code)\n",
    "    token_stream = generate_tokens(StringIO(code).readline)\n",
    "    tokens = []\n",
    "    indent_level = 0\n",
    "    new_line = False\n",
    "\n",
    "    for toknum, tokval, (srow, scol), (erow, ecol), _ in token_stream:\n",
    "        if toknum == tk.NEWLINE:\n",
    "            tokens.append('#NEWLINE#')\n",
    "            new_line = True\n",
    "        elif toknum == tk.INDENT:\n",
    "            indent_level += 1\n",
    "            # new_line = False\n",
    "            # for i in range(indent_level):\n",
    "            #     tokens.append('#INDENT#')\n",
    "        elif toknum == tk.STRING:\n",
    "            tokens.append(tokval.replace(' ', '#SPACE#').replace('\\t', '#TAB#').replace('\\r\\n', '#NEWLINE#').replace('\\n', '#NEWLINE#'))\n",
    "        elif toknum == tk.DEDENT:\n",
    "            indent_level -= 1\n",
    "            # for i in range(indent_level):\n",
    "            #     tokens.append('#INDENT#')\n",
    "            # new_line = False\n",
    "        else:\n",
    "            tokval = tokval.replace('\\n', '#NEWLINE#')\n",
    "            if new_line:\n",
    "                for i in range(indent_level):\n",
    "                    tokens.append('#INDENT#')\n",
    "\n",
    "            new_line = False\n",
    "            tokens.append(tokval)\n",
    "\n",
    "    # remove ending None\n",
    "    if len(tokens[-1]) == 0:\n",
    "        tokens = tokens[:-1]\n",
    "\n",
    "    if '\\n' in tokval:\n",
    "        pass\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokenize(code):\n",
    "    token_stream = generate_tokens(StringIO(code).readline)\n",
    "    tokens = []\n",
    "    for toknum, tokval, (srow, scol), (erow, ecol), _ in token_stream:\n",
    "        if toknum == tk.ENDMARKER:\n",
    "            break\n",
    "\n",
    "        tokens.append(tokval)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def compare_ast(node1, node2):\n",
    "    # Python 3\n",
    "    # if not isinstance(node1, str) and not isinstance(node1, unicode):\n",
    "    if not isinstance(node1, str):\n",
    "        if type(node1) is not type(node2):\n",
    "            return False\n",
    "    if isinstance(node1, ast.AST):\n",
    "        for k, v in list(vars(node1).items()):\n",
    "            if k in ('lineno', 'col_offset', 'ctx'):\n",
    "                continue\n",
    "            if not compare_ast(v, getattr(node2, k)):\n",
    "                return False\n",
    "        return True\n",
    "    elif isinstance(node1, list):\n",
    "        return all(itertools.starmap(compare_ast, zip(node1, node2)))\n",
    "    else:\n",
    "        return node1 == node2\n",
    "\n",
    "\n",
    "def encoded_code_tokens_to_code(encoded_tokens, indent=' '):\n",
    "    decoded_tokens = []\n",
    "    for i in range(len(encoded_tokens)):\n",
    "        token = encoded_tokens[i]\n",
    "        token = token.replace('#TAB#', '\\t').replace('#SPACE#', ' ')\n",
    "\n",
    "        if token == '#INDENT#': decoded_tokens.append(indent)\n",
    "        elif token == '#NEWLINE#': decoded_tokens.append('\\n')\n",
    "        else:\n",
    "            token = token.replace('#NEWLINE#', '\\n')\n",
    "            decoded_tokens.append(token)\n",
    "            decoded_tokens.append(' ')\n",
    "\n",
    "    code = ''.join(decoded_tokens).strip()\n",
    "\n",
    "    return code\n",
    "\n",
    "\n",
    "def find_sub_sequence(sequence, query_seq):\n",
    "    for i in range(len(sequence)):\n",
    "        if sequence[i: len(query_seq) + i] == query_seq:\n",
    "            return i, len(query_seq) + i\n",
    "\n",
    "    raise IndexError\n",
    "\n",
    "\n",
    "def replace_sequence(sequence, old_seq, new_seq):\n",
    "    matched = False\n",
    "    for i in range(len(sequence)):\n",
    "        if sequence[i: i + len(old_seq)] == old_seq:\n",
    "            matched = True\n",
    "            sequence[i:i + len(old_seq)] = new_seq\n",
    "    return matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and clean data\n",
    "def read_clean_dataset(dataset_path, output_path):\n",
    "    train = json.load(open(dataset_path))\n",
    "\n",
    "    for i, example in enumerate(train):\n",
    "        # updating `train` in place\n",
    "        intent = example['intent']\n",
    "\n",
    "        rewritten_intent = example['rewritten_intent']\n",
    "\n",
    "        snippet = example['snippet']\n",
    "        # print(i)\n",
    "        # code_tokens = get_encoded_code_tokens(snippet)\n",
    "        # print(' '.join(code_tokens))\n",
    "\n",
    "        failed = False\n",
    "        intent_tokens = []\n",
    "        if rewritten_intent:\n",
    "            try:\n",
    "                canonical_intent, slot_map = canonicalize_intent(rewritten_intent)\n",
    "                #print(canonical_intent, slot_map)\n",
    "\n",
    "                snippet = snippet\n",
    "                canonical_snippet = canonicalize_code(snippet, slot_map)\n",
    "                #print(\"canonical_snippet:\", canonical_snippet, slot_map)\n",
    "\n",
    "                intent_tokens = nltk.word_tokenize(canonical_intent)\n",
    "\n",
    "                decanonical_snippet = decanonicalize_code(canonical_snippet, slot_map)\n",
    "                #print(\"decanonical_snippet: \",decanonical_snippet)\n",
    "\n",
    "                snippet_reconstr = astor.to_source(ast.parse(snippet)).strip()\n",
    "                #print(\"snippet_reconstr: \",decanonical_snippet)\n",
    "\n",
    "                decanonical_snippet_reconstr = astor.to_source(ast.parse(decanonical_snippet)).strip()\n",
    "                #print(\"decanonical_snippet_reconstr: \",decanonical_snippet_reconstr)\n",
    "                encoded_reconstr_code = get_encoded_code_tokens(decanonical_snippet_reconstr)\n",
    "                decoded_reconstr_code = encoded_code_tokens_to_code(encoded_reconstr_code)\n",
    "\n",
    "                # syntax error in snippet\n",
    "                if not compare_ast(ast.parse(decoded_reconstr_code), ast.parse(snippet)):\n",
    "                    print(i)\n",
    "                    print('Original Snippet: %s' % snippet_reconstr)\n",
    "                    print('Tokenized Snippet: %s' % ' '.join(encoded_reconstr_code))\n",
    "                    print('decoded_reconstr_code: %s' % decoded_reconstr_code)\n",
    "\n",
    "            except:\n",
    "                print('*' * 20, file=sys.stderr)\n",
    "                print(i, file=sys.stderr)\n",
    "                print(intent, file=sys.stderr)\n",
    "                print(snippet, file=sys.stderr)\n",
    "                traceback.print_exc()\n",
    "\n",
    "                failed = True\n",
    "            finally:\n",
    "                example['slot_map'] = slot_map\n",
    "\n",
    "        if rewritten_intent is None:\n",
    "            encoded_reconstr_code = get_encoded_code_tokens(snippet.strip())\n",
    "        else:\n",
    "            encoded_reconstr_code = get_encoded_code_tokens(canonical_snippet.strip())\n",
    "\n",
    "        if not intent_tokens:\n",
    "            intent_tokens = nltk.word_tokenize(intent)\n",
    "\n",
    "        example['intent_tokens'] = intent_tokens\n",
    "        example['snippet_tokens'] = encoded_reconstr_code\n",
    "\n",
    "    json.dump(train, open(output_path, 'w'), indent=2)\n",
    "read_clean_dataset(train_path, train_clean_output_path)\n",
    "read_clean_dataset(test_path, test_clean_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fasttext Word Embedding on both *Intent* and *Snippet*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = json.load(open(train_clean_output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_keywords=[] # a list of lists of keywords\n",
    "for i, example in enumerate(train_clean):\n",
    "    # intent_tokens is from rewritten_intent\n",
    "    list_keywords.append(example[\"intent_tokens\"]+example[\"snippet_tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size= 500\n",
    "window_size= 20\n",
    "min_count= 1\n",
    "epoch = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time: 89.87186884880066 s\n"
     ]
    }
   ],
   "source": [
    "st=time()\n",
    "#[TODO] tuning hyperparameters\n",
    "model = FastText(size=vocab_size, window=window_size, min_count=min_count)  # instantiate\n",
    "model.build_vocab(sentences=list_keywords)\n",
    "model.train(sentences=list_keywords, total_examples=len(list_keywords), epochs=epoch)  # train\n",
    "print(\"Run time: {} s\".format(time()-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vectors to file if you want to use them later\n",
    "trained_ft_vectors = model.wv\n",
    "trained_ft_vectors.save_word2vec_format(wordembedding_file_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time: 2.1812071800231934 s\n"
     ]
    }
   ],
   "source": [
    "# load saved wm\n",
    "st=time()\n",
    "trained_ft_vectors = KeyedVectors.load_word2vec_format(wordembedding_file_path)\n",
    "print(\"Run time: {} s\".format(time()-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word size: 5177\n",
      "dimension: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/waynewu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(\"word size:\", len(trained_ft_vectors.wv.vocab))\n",
    "print(\"dimension:\", trained_ft_vectors.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/waynewu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ndarray', 0.8250784873962402),\n",
       " ('arrays', 0.8035600185394287),\n",
       " ('intarray', 0.7503020763397217),\n",
       " ('getarray', 0.6716115474700928),\n",
       " ('image_array', 0.6594366431236267),\n",
       " ('order_array', 0.6028176546096802),\n",
       " ('Rearrange', 0.507854700088501),\n",
       " ('imageArray', 0.49495428800582886),\n",
       " ('rearrange', 0.4916466474533081),\n",
       " ('myArray', 0.49076616764068604)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST \n",
    "trained_ft_vectors.wv.most_similar('array')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check cosine similarity between `intent_tokens` and `snippet_tokens` in training set \n",
    "should be high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize a word represenatation vector that its L2 norm is 1.\n",
    "# we do this so that the cosine similarity reduces to a simple dot product\n",
    "\n",
    "def normalize(word_representations):\n",
    "    for word in word_representations:\n",
    "        total=0\n",
    "        for key in word_representations[word]:\n",
    "            total+=word_representations[word][key]*word_representations[word][key]\n",
    "            \n",
    "        total=math.sqrt(total)\n",
    "        for key in word_representations[word]:\n",
    "            word_representations[word][key]/=total\n",
    "\n",
    "def dictionary_dot_product(dict1, dict2):\n",
    "    dot=0\n",
    "    for key in dict1:\n",
    "        if key in dict2:\n",
    "            dot+=dict1[key]*dict2[key]\n",
    "    return dot\n",
    "\n",
    "def find_sim(word_representations, query):\n",
    "    if query not in word_representations:\n",
    "        print(\"'%s' is not in vocabulary\" % query)\n",
    "        return None\n",
    "    \n",
    "    scores={}\n",
    "    for word in word_representations:\n",
    "        cosine=dictionary_dot_product(word_representations[query], word_representations[word])\n",
    "        scores[word]=cosine\n",
    "    return scores\n",
    "\n",
    "# Find the K words with highest cosine similarity to a query in a set of word_representations\n",
    "def find_nearest_neighbors(word_representations, query, K):\n",
    "    scores=find_sim(word_representations, query)\n",
    "    if scores != None:\n",
    "        sorted_x = sorted(scores.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        for idx, (k, v) in enumerate(sorted_x[:K]):\n",
    "            print(\"%s\\t%s\\t%.5f\" % (idx,k,v))\n",
    "            \n",
    "def get_most_relevant_document(tokenized_ques, word_embedding, doc_embedding, num=10):\n",
    "    \"\"\"Return the functions that are most relevant to the natual language question.\n",
    "\n",
    "    Args:\n",
    "        tokenized_ques: A list. \n",
    "        word_embedding: Word embedding generated from codebase.\n",
    "        doc_embedding: Document embedding generated from codebase\n",
    "        num: The number of top similar functions to return.\n",
    "\n",
    "    Returns:\n",
    "        A list of indices of the top NUM related functions to the QUESTION in the WORD_EMBEDDING.\n",
    "    \n",
    "    \"\"\"\n",
    "    vec_ques=np.zeros((1,doc_embedding.shape[1])) #vocab_size\n",
    "    token_count=0\n",
    "    has_token_in_embedding=False\n",
    "    for token in tokenized_ques:\n",
    "        if token in word_embedding:\n",
    "            has_token_in_embedding=True\n",
    "            vec_ques+=word_embedding[token]\n",
    "            token_count+=1\n",
    "    \n",
    "    if has_token_in_embedding:\n",
    "        mean_vec_ques=vec_ques/token_count\n",
    "    \n",
    "    \n",
    "        # compute similarity between this question and each of the source code snippets\n",
    "        cosine_sim=[]\n",
    "        for idx, doc in enumerate(doc_embedding):\n",
    "            #[TODO] fix dimension\n",
    "            try:\n",
    "                cosine_sim.append(cosine_similarity(mean_vec_ques, doc.reshape(1, -1))[0][0])\n",
    "            except ValueError:\n",
    "                print(question)\n",
    "                print(vec_ques, token_count)\n",
    "                print(mean_vec_ques)\n",
    "                print(doc.reshape(1, -1))\n",
    "        #print(cosine_sim)\n",
    "        # get top `num` similar functions\n",
    "        result=np.array(cosine_sim).argsort()[-num:][::-1]\n",
    "    else:\n",
    "        result=np.nan\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (10 result)\n",
    "\n",
    "Are the real match in the top 10 code snippet?<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time: 0.8772883415222168 s\n"
     ]
    }
   ],
   "source": [
    "# build document embedding on `code` only\n",
    "st=time()\n",
    "train_size=len(train_clean)\n",
    "train_ques_list=[] # [{\"question_id\": int, \"intent_tokens\": [...]}, ...]\n",
    "train_document_embeddings=np.zeros((train_size, vocab_size))\n",
    "\n",
    "for idx, example in enumerate(train_clean):\n",
    "    doc_vec_sum=np.zeros(vocab_size)\n",
    "    train_ques_list.append({\"question_id\": example[\"question_id\"], \"intent_tokens\": example[\"intent_tokens\"]})\n",
    "    for term in example[\"snippet_tokens\"]:\n",
    "        if term in trained_ft_vectors:\n",
    "            doc_vec_sum+=trained_ft_vectors[term]\n",
    "    \n",
    "    train_document_embeddings[idx]=doc_vec_sum/len(example[\"snippet_tokens\"])\n",
    "    \n",
    "# save the whole document_embeddings\n",
    "np.savetxt(docembedding_file_path, train_document_embeddings, delimiter=\",\")\n",
    "print(\"Run time: {} s\".format(time()-st))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time: 696.478753566742 s\n"
     ]
    }
   ],
   "source": [
    "st=time()\n",
    "train_list_most_relevant_doc=[] #[{\"question_id\": int, \"similar\": [id_in_train_clean]}]\n",
    "\n",
    "for idx in range(len(train_ques_list)): \n",
    "    question_token_list=train_ques_list[idx][\"intent_tokens\"]\n",
    "    question_id=train_ques_list[idx][\"question_id\"]\n",
    "    \n",
    "    most_relevant_doc=get_most_relevant_document(question_token_list, trained_ft_vectors, train_document_embeddings)\n",
    "    #if question_id in list_most_relevant_doc:\n",
    "        # this exist\n",
    "    train_list_most_relevant_doc.append({\"question_id\": question_id, \"similar\": most_relevant_doc})\n",
    "print(\"Run time: {} s\".format(time()-st)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.1139134089953762\n",
      "average position: 3.5608856088560885\n"
     ]
    }
   ],
   "source": [
    "### if one of the ten results exactly match the code snippet, ct +=1 \n",
    "ct = 0\n",
    "### the correct result's ranking in 10 list \n",
    "position_list = []\n",
    "\n",
    "for ques_sim_dict in train_list_most_relevant_doc:\n",
    "    if np.isnan(ques_sim_dict['similar']).any() == False:\n",
    "        for i, idx in enumerate(ques_sim_dict['similar']):\n",
    "            if np.isnan(idx) == False:\n",
    "                 if train_ques_list[idx]['question_id'] == ques_sim_dict['question_id']:\n",
    "                    ct += 1\n",
    "                    position_list.append(i)\n",
    "\n",
    "print(\"precision:\", ct/len(train_ques_list))\n",
    "print(\"average position:\", np.mean(position_list))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean = json.load(open(test_clean_output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time: 0.06093740463256836 s\n"
     ]
    }
   ],
   "source": [
    "# build document embedding on `code` only\n",
    "st=time()\n",
    "test_size=len(test_clean)\n",
    "test_ques_list=[] # [{\"question_id\": int, \"intent_tokens\": [...]}, ...]\n",
    "test_document_embeddings=np.zeros((test_size, vocab_size))\n",
    "\n",
    "for idx, example in enumerate(test_clean):\n",
    "    doc_vec_sum=np.zeros(vocab_size)\n",
    "    test_ques_list.append({\"question_id\": example[\"question_id\"], \"intent_tokens\": example[\"intent_tokens\"]})\n",
    "    for term in example[\"snippet_tokens\"]:\n",
    "        if term in trained_ft_vectors:\n",
    "            doc_vec_sum+=trained_ft_vectors[term]\n",
    "    \n",
    "    test_document_embeddings[idx]=doc_vec_sum/len(example[\"snippet_tokens\"])\n",
    "print(\"Run time: {} s\".format(time()-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time: 30.589938640594482 s\n"
     ]
    }
   ],
   "source": [
    "# np.savetxt(docembedding_file_path, document_embeddings, delimiter=\",\")\n",
    "\n",
    "st=time()\n",
    "test_list_most_relevant_doc=[] #[{\"question_id\": int, \"similar\": [id_in_train_clean]}]\n",
    "for idx in range(len(test_ques_list)): \n",
    "    question_token_list=test_ques_list[idx][\"intent_tokens\"]\n",
    "    question_id=test_ques_list[idx][\"question_id\"]\n",
    "    \n",
    "    most_relevant_doc=get_most_relevant_document(question_token_list, trained_ft_vectors, test_document_embeddings)\n",
    "    test_list_most_relevant_doc.append({\"question_id\": question_id, \"similar\": most_relevant_doc})\n",
    "    \n",
    "print(\"Run time: {} s\".format(time()-st)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.27\n",
      "average position: 3.185185185185185\n"
     ]
    }
   ],
   "source": [
    "### if one of the ten results exactly match the code snippet, ct +=1 \n",
    "ct = 0\n",
    "### the correct result's ranking in 10 list \n",
    "position_list = []\n",
    "\n",
    "for ques_sim_dict in test_list_most_relevant_doc:\n",
    "    for i, idx in enumerate(ques_sim_dict['similar']):\n",
    "        if test_ques_list[idx]['question_id'] == ques_sim_dict['question_id']:\n",
    "            ct += 1\n",
    "            position_list.append(i)\n",
    "            \n",
    "print(\"precision:\", ct/len(test_ques_list))\n",
    "print(\"average position:\", np.mean(position_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time: 0.06258440017700195 s\n",
      "Run time: 31.271640062332153 s\n"
     ]
    }
   ],
   "source": [
    "# build document embedding on `code` only\n",
    "st=time()\n",
    "test_size=len(test_clean)\n",
    "test_ques_list=[] # [{\"question_id\": int, \"intent_tokens\": [...]}, ...]\n",
    "test_document_embeddings=np.zeros((test_size, vocab_size))\n",
    "\n",
    "for idx, example in enumerate(test_clean):\n",
    "    doc_vec_sum=np.zeros(vocab_size)\n",
    "    test_ques_list.append({\"question_id\": example[\"question_id\"], \"intent_tokens\": example[\"intent_tokens\"]})\n",
    "    for term in example[\"snippet_tokens\"]:\n",
    "        if term in model.wv:\n",
    "            doc_vec_sum+=model.wv[term]\n",
    "    \n",
    "    test_document_embeddings[idx]=doc_vec_sum/len(example[\"snippet_tokens\"])\n",
    "print(\"Run time: {} s\".format(time()-st))\n",
    "\n",
    "\n",
    "# np.savetxt(docembedding_file_path, document_embeddings, delimiter=\",\")\n",
    "\n",
    "st=time()\n",
    "test_list_most_relevant_doc=[] #[{\"question_id\": int, \"similar\": [id_in_train_clean]}]\n",
    "for idx in range(len(test_ques_list)): \n",
    "    question_token_list=test_ques_list[idx][\"intent_tokens\"]\n",
    "    question_id=test_ques_list[idx][\"question_id\"]\n",
    "    \n",
    "    most_relevant_doc=get_most_relevant_document(question_token_list, model.wv, test_document_embeddings)\n",
    "    test_list_most_relevant_doc.append({\"question_id\": question_id, \"similar\": most_relevant_doc})\n",
    "    \n",
    "print(\"Run time: {} s\".format(time()-st)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.262\n",
      "average position: 3.5801526717557253\n"
     ]
    }
   ],
   "source": [
    "### if one of the ten results exactly match the code snippet, ct +=1 \n",
    "ct = 0\n",
    "### the correct result's ranking in 10 list \n",
    "position_list = []\n",
    "\n",
    "for ques_sim_dict in test_list_most_relevant_doc:\n",
    "    for i, idx in enumerate(ques_sim_dict['similar']):\n",
    "        if test_ques_list[idx]['question_id'] == ques_sim_dict['question_id']:\n",
    "            ct += 1\n",
    "            position_list.append(i)\n",
    "            \n",
    "print(\"precision:\", ct/len(test_ques_list))\n",
    "print(\"average position:\", np.mean(position_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TO DO] try other embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
