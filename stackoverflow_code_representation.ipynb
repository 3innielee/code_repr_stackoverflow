{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "[Curated Stack Overflow pair of intent and code snippet](https://conala-corpus.github.io/)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import nltk\n",
    "import traceback\n",
    "import astor\n",
    "import token as tk\n",
    "from tokenize import generate_tokens\n",
    "from io import StringIO\n",
    "import itertools \n",
    "from gensim.models import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "from time import time\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "train_path=\"data/conala-corpus/conala-train.json\"\n",
    "test_path=\"data/conala-corpus/conala-test.json\"\n",
    "train_clean_output_path=\"data/conala-corpus/.train.seq2seq\"\n",
    "test_clean_output_path=\"data/conala-corpus/.test.seq2seq\"\n",
    "wordembedding_file_path=\"data/conala-corpus/embeddings.txt\"\n",
    "docembedding_file_path=\"data/document_embeddings.csv\"\n",
    "\n",
    "vocab_size=200\n",
    "window_size=5\n",
    "min_count=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUOTED_STRING_RE = re.compile(r\"(?P<quote>[`'\\\"])(?P<string>.*?)(?P=quote)\")\n",
    "\n",
    "\n",
    "def canonicalize_intent(intent):\n",
    "    str_matches = QUOTED_STRING_RE.findall(intent)\n",
    "\n",
    "    slot_map = dict()\n",
    "\n",
    "    return intent, slot_map\n",
    "\n",
    "\n",
    "def replace_strings_in_ast(py_ast, string2slot):\n",
    "    for node in ast.walk(py_ast):\n",
    "        for k, v in list(vars(node).items()):\n",
    "            if k in ('lineno', 'col_offset', 'ctx'):\n",
    "                continue\n",
    "            # Python 3\n",
    "            # if isinstance(v, str) or isinstance(v, unicode):\n",
    "            if isinstance(v, str):\n",
    "                if v in string2slot:\n",
    "                    val = string2slot[v]\n",
    "                    # Python 3\n",
    "                    # if isinstance(val, unicode):\n",
    "                    #     try: val = val.encode('ascii')\n",
    "                    #     except: pass\n",
    "                    setattr(node, k, val)\n",
    "                else:\n",
    "                    # Python 3\n",
    "                    # if isinstance(v, str):\n",
    "                    #     str_key = unicode(v)\n",
    "                    # else:\n",
    "                    #     str_key = v.encode('utf-8')\n",
    "                    str_key = v\n",
    "\n",
    "                    if str_key in string2slot:\n",
    "                        val = string2slot[str_key]\n",
    "                        if isinstance(val, str):\n",
    "                            try: val = val.encode('ascii')\n",
    "                            except: pass\n",
    "                        setattr(node, k, val)\n",
    "\n",
    "\n",
    "def canonicalize_code(code, slot_map):\n",
    "    string2slot = {x[1]['value']: x[0] for x in list(slot_map.items())}\n",
    "\n",
    "    py_ast = ast.parse(code)\n",
    "    replace_strings_in_ast(py_ast, string2slot)\n",
    "    canonical_code = astor.to_source(py_ast)\n",
    "\n",
    "    return canonical_code\n",
    "\n",
    "\n",
    "def decanonicalize_code(code, slot_map):\n",
    "    try:\n",
    "        slot2string = {x[0]: x[1]['value'] for x in list(slot_map.items())}\n",
    "        py_ast = ast.parse(code)\n",
    "        replace_strings_in_ast(py_ast, slot2string)\n",
    "        raw_code = astor.to_source(py_ast)\n",
    "      # for slot_name, slot_info in slot_map.items():\n",
    "      #     raw_code = raw_code.replace(slot_name, slot_info['value'])\n",
    "\n",
    "        return raw_code.strip()\n",
    "    except:\n",
    "        return code\n",
    "\n",
    "def detokenize_code(code_tokens):\n",
    "    newline_pos = [i for i, x in enumerate(code_tokens) if x == '\\n']\n",
    "    newline_pos.append(len(code_tokens))\n",
    "    start = 0\n",
    "    lines = []\n",
    "    for i in newline_pos:\n",
    "        line = ' '.join(code_tokens[start: i])\n",
    "        start = i + 1\n",
    "        lines.append(line)\n",
    "\n",
    "    code = '\\n'.join(lines).strip()\n",
    "\n",
    "    return code\n",
    "\n",
    "\n",
    "def encode_tokenized_code(code_tokens):\n",
    "    tokens = []\n",
    "    for token in code_tokens:\n",
    "        if token == '\\t':\n",
    "            tokens.append('_TAB_')\n",
    "        elif token == '\\n':\n",
    "            tokens.append('_NEWLINE_')\n",
    "\n",
    "\n",
    "def get_encoded_code_tokens(code):\n",
    "    code = code.strip()\n",
    "    #print(code)\n",
    "    token_stream = generate_tokens(StringIO(code).readline)\n",
    "    tokens = []\n",
    "    indent_level = 0\n",
    "    new_line = False\n",
    "\n",
    "    for toknum, tokval, (srow, scol), (erow, ecol), _ in token_stream:\n",
    "        if toknum == tk.NEWLINE:\n",
    "            tokens.append('#NEWLINE#')\n",
    "            new_line = True\n",
    "        elif toknum == tk.INDENT:\n",
    "            indent_level += 1\n",
    "            # new_line = False\n",
    "            # for i in range(indent_level):\n",
    "            #     tokens.append('#INDENT#')\n",
    "        elif toknum == tk.STRING:\n",
    "            tokens.append(tokval.replace(' ', '#SPACE#').replace('\\t', '#TAB#').replace('\\r\\n', '#NEWLINE#').replace('\\n', '#NEWLINE#'))\n",
    "        elif toknum == tk.DEDENT:\n",
    "            indent_level -= 1\n",
    "            # for i in range(indent_level):\n",
    "            #     tokens.append('#INDENT#')\n",
    "            # new_line = False\n",
    "        else:\n",
    "            tokval = tokval.replace('\\n', '#NEWLINE#')\n",
    "            if new_line:\n",
    "                for i in range(indent_level):\n",
    "                    tokens.append('#INDENT#')\n",
    "\n",
    "            new_line = False\n",
    "            tokens.append(tokval)\n",
    "\n",
    "    # remove ending None\n",
    "    if len(tokens[-1]) == 0:\n",
    "        tokens = tokens[:-1]\n",
    "\n",
    "    if '\\n' in tokval:\n",
    "        pass\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokenize(code):\n",
    "    token_stream = generate_tokens(StringIO(code).readline)\n",
    "    tokens = []\n",
    "    for toknum, tokval, (srow, scol), (erow, ecol), _ in token_stream:\n",
    "        if toknum == tk.ENDMARKER:\n",
    "            break\n",
    "\n",
    "        tokens.append(tokval)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def compare_ast(node1, node2):\n",
    "    # Python 3\n",
    "    # if not isinstance(node1, str) and not isinstance(node1, unicode):\n",
    "    if not isinstance(node1, str):\n",
    "        if type(node1) is not type(node2):\n",
    "            return False\n",
    "    if isinstance(node1, ast.AST):\n",
    "        for k, v in list(vars(node1).items()):\n",
    "            if k in ('lineno', 'col_offset', 'ctx'):\n",
    "                continue\n",
    "            if not compare_ast(v, getattr(node2, k)):\n",
    "                return False\n",
    "        return True\n",
    "    elif isinstance(node1, list):\n",
    "        return all(itertools.starmap(compare_ast, zip(node1, node2)))\n",
    "    else:\n",
    "        return node1 == node2\n",
    "\n",
    "\n",
    "def encoded_code_tokens_to_code(encoded_tokens, indent=' '):\n",
    "    decoded_tokens = []\n",
    "    for i in range(len(encoded_tokens)):\n",
    "        token = encoded_tokens[i]\n",
    "        token = token.replace('#TAB#', '\\t').replace('#SPACE#', ' ')\n",
    "\n",
    "        if token == '#INDENT#': decoded_tokens.append(indent)\n",
    "        elif token == '#NEWLINE#': decoded_tokens.append('\\n')\n",
    "        else:\n",
    "            token = token.replace('#NEWLINE#', '\\n')\n",
    "            decoded_tokens.append(token)\n",
    "            decoded_tokens.append(' ')\n",
    "\n",
    "    code = ''.join(decoded_tokens).strip()\n",
    "\n",
    "    return code\n",
    "\n",
    "\n",
    "def find_sub_sequence(sequence, query_seq):\n",
    "    for i in range(len(sequence)):\n",
    "        if sequence[i: len(query_seq) + i] == query_seq:\n",
    "            return i, len(query_seq) + i\n",
    "\n",
    "    raise IndexError\n",
    "\n",
    "\n",
    "def replace_sequence(sequence, old_seq, new_seq):\n",
    "    matched = False\n",
    "    for i in range(len(sequence)):\n",
    "        if sequence[i: i + len(old_seq)] == old_seq:\n",
    "            matched = True\n",
    "            sequence[i:i + len(old_seq)] = new_seq\n",
    "    return matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and clean data\n",
    "def read_clean_dataset(dataset_path, output_path):\n",
    "    train = json.load(open(dataset_path))\n",
    "\n",
    "    for i, example in enumerate(train):\n",
    "        # updating `train` in place\n",
    "        intent = example['intent']\n",
    "\n",
    "        rewritten_intent = example['rewritten_intent']\n",
    "\n",
    "        snippet = example['snippet']\n",
    "        # print(i)\n",
    "        # code_tokens = get_encoded_code_tokens(snippet)\n",
    "        # print(' '.join(code_tokens))\n",
    "\n",
    "        failed = False\n",
    "        intent_tokens = []\n",
    "        if rewritten_intent:\n",
    "            try:\n",
    "                canonical_intent, slot_map = canonicalize_intent(rewritten_intent)\n",
    "                #print(canonical_intent, slot_map)\n",
    "\n",
    "                snippet = snippet\n",
    "                canonical_snippet = canonicalize_code(snippet, slot_map)\n",
    "                #print(\"canonical_snippet:\", canonical_snippet, slot_map)\n",
    "\n",
    "                intent_tokens = nltk.word_tokenize(canonical_intent)\n",
    "\n",
    "                decanonical_snippet = decanonicalize_code(canonical_snippet, slot_map)\n",
    "                #print(\"decanonical_snippet: \",decanonical_snippet)\n",
    "\n",
    "                snippet_reconstr = astor.to_source(ast.parse(snippet)).strip()\n",
    "                #print(\"snippet_reconstr: \",decanonical_snippet)\n",
    "\n",
    "                decanonical_snippet_reconstr = astor.to_source(ast.parse(decanonical_snippet)).strip()\n",
    "                #print(\"decanonical_snippet_reconstr: \",decanonical_snippet_reconstr)\n",
    "                encoded_reconstr_code = get_encoded_code_tokens(decanonical_snippet_reconstr)\n",
    "                decoded_reconstr_code = encoded_code_tokens_to_code(encoded_reconstr_code)\n",
    "\n",
    "                # syntax error in snippet\n",
    "                if not compare_ast(ast.parse(decoded_reconstr_code), ast.parse(snippet)):\n",
    "                    print(i)\n",
    "                    print('Original Snippet: %s' % snippet_reconstr)\n",
    "                    print('Tokenized Snippet: %s' % ' '.join(encoded_reconstr_code))\n",
    "                    print('decoded_reconstr_code: %s' % decoded_reconstr_code)\n",
    "\n",
    "            except:\n",
    "                print('*' * 20, file=sys.stderr)\n",
    "                print(i, file=sys.stderr)\n",
    "                print(intent, file=sys.stderr)\n",
    "                print(snippet, file=sys.stderr)\n",
    "                traceback.print_exc()\n",
    "\n",
    "                failed = True\n",
    "            finally:\n",
    "                example['slot_map'] = slot_map\n",
    "\n",
    "        if rewritten_intent is None:\n",
    "            encoded_reconstr_code = get_encoded_code_tokens(snippet.strip())\n",
    "        else:\n",
    "            encoded_reconstr_code = get_encoded_code_tokens(canonical_snippet.strip())\n",
    "\n",
    "        if not intent_tokens:\n",
    "            intent_tokens = nltk.word_tokenize(intent)\n",
    "\n",
    "        example['intent_tokens'] = intent_tokens\n",
    "        example['snippet_tokens'] = encoded_reconstr_code\n",
    "\n",
    "    json.dump(train, open(output_path, 'w'), indent=2)\n",
    "read_clean_dataset(train_path, train_clean_output_path)\n",
    "read_clean_dataset(test_path, test_clean_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fasttext Word Embedding on both *Intent* and *Snippet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = json.load(open(train_clean_output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_keywords=[] # a list of lists of keywords\n",
    "for i, example in enumerate(train_clean):\n",
    "    # intent_tokens is from rewritten_intent\n",
    "    list_keywords.append(set(example[\"intent_tokens\"]+example[\"snippet_tokens\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st=time()\n",
    "#[TODO] tuning hyperparameters\n",
    "model = FastText(size=vocab_size, window=window_size, min_count=min_count)  # instantiate\n",
    "model.build_vocab(sentences=list_keywords)\n",
    "model.train(sentences=list_keywords, total_examples=len(list_keywords), epochs=10)  # train\n",
    "print(\"Run time: {} s\".format(time()-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vectors to file if you want to use them later\n",
    "trained_ft_vectors = model.wv\n",
    "trained_ft_vectors.save_word2vec_format(wordembedding_file_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved wm\n",
    "st=time()\n",
    "trained_ft_vectors = KeyedVectors.load_word2vec_format(wordembedding_file_path)\n",
    "print(\"Run time: {} s\".format(time()-st))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check cosine similarity between `intent_tokens` and `snippet_tokens` in training set \n",
    "should be high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build document embedding on `code` only\n",
    "st=time()\n",
    "train_size=len(train_clean)\n",
    "train_ques_list=[] # [{\"question_id\": int, \"intent_tokens\": [...]}, ...]\n",
    "document_embeddings=np.zeros((train_size, vocab_size))\n",
    "for idx, example in enumerate(train_clean):\n",
    "    doc_vec_sum=np.zeros(vocab_size)\n",
    "    train_ques_list.append({\"question_id\": example[\"question_id\"], \"intent_tokens\": example[\"intent_tokens\"]})\n",
    "    for term in example[\"snippet_tokens\"]:\n",
    "        doc_vec_sum+=trained_ft_vectors[term]\n",
    "    \n",
    "    document_embeddings[idx]=doc_vec_sum/len(example[\"snippet_tokens\"])\n",
    "    \n",
    "# save the whole document_embeddings\n",
    "np.savetxt(docembedding_file_path, document_embeddings, delimiter=\",\")\n",
    "print(\"Run time: {} s\".format(time()-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize a word represenatation vector that its L2 norm is 1.\n",
    "# we do this so that the cosine similarity reduces to a simple dot product\n",
    "\n",
    "def normalize(word_representations):\n",
    "    for word in word_representations:\n",
    "        total=0\n",
    "        for key in word_representations[word]:\n",
    "            total+=word_representations[word][key]*word_representations[word][key]\n",
    "            \n",
    "        total=math.sqrt(total)\n",
    "        for key in word_representations[word]:\n",
    "            word_representations[word][key]/=total\n",
    "\n",
    "def dictionary_dot_product(dict1, dict2):\n",
    "    dot=0\n",
    "    for key in dict1:\n",
    "        if key in dict2:\n",
    "            dot+=dict1[key]*dict2[key]\n",
    "    return dot\n",
    "\n",
    "def find_sim(word_representations, query):\n",
    "    if query not in word_representations:\n",
    "        print(\"'%s' is not in vocabulary\" % query)\n",
    "        return None\n",
    "    \n",
    "    scores={}\n",
    "    for word in word_representations:\n",
    "        cosine=dictionary_dot_product(word_representations[query], word_representations[word])\n",
    "        scores[word]=cosine\n",
    "    return scores\n",
    "\n",
    "# Find the K words with highest cosine similarity to a query in a set of word_representations\n",
    "def find_nearest_neighbors(word_representations, query, K):\n",
    "    scores=find_sim(word_representations, query)\n",
    "    if scores != None:\n",
    "        sorted_x = sorted(scores.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        for idx, (k, v) in enumerate(sorted_x[:K]):\n",
    "            print(\"%s\\t%s\\t%.5f\" % (idx,k,v))\n",
    "            \n",
    "def get_most_relevant_document(tokenized_ques, word_embedding, doc_embedding, num=10):\n",
    "    \"\"\"Return the functions that are most relevant to the natual language question.\n",
    "\n",
    "    Args:\n",
    "        tokenized_ques: A list. \n",
    "        word_embedding: Word embedding generated from codebase.\n",
    "        doc_embedding: Document embedding generated from codebase\n",
    "        num: The number of top similar functions to return.\n",
    "\n",
    "    Returns:\n",
    "        A list of indices of the top NUM related functions to the QUESTION in the WORD_EMBEDDING.\n",
    "    \n",
    "    \"\"\"\n",
    "    vec_ques=np.zeros((1,document_embeddings.shape[1])) #vocab_size\n",
    "    token_count=0\n",
    "    has_token_in_embedding=False\n",
    "    for token in tokenized_ques:\n",
    "        if token in word_embedding:\n",
    "            has_token_in_embedding=True\n",
    "            vec_ques+=word_embedding[token]\n",
    "            token_count+=1\n",
    "    \n",
    "    if has_token_in_embedding:\n",
    "        mean_vec_ques=vec_ques/token_count\n",
    "    \n",
    "    \n",
    "        # compute similarity between this question and each of the source code snippets\n",
    "        cosine_sim=[]\n",
    "        for idx, doc in enumerate(document_embeddings):\n",
    "            #[TODO] fix dimension\n",
    "\n",
    "            try:\n",
    "                cosine_sim.append(cosine_similarity(mean_vec_ques, doc.reshape(1, -1))[0][0])\n",
    "            except ValueError:\n",
    "                print(question)\n",
    "                print(vec_ques, token_count)\n",
    "                print(mean_vec_ques)\n",
    "                print(doc.reshape(1, -1))\n",
    "        #print(cosine_sim)\n",
    "        # get top `num` similar functions\n",
    "        result=np.array(cosine_sim).argsort()[-num:][::-1]\n",
    "    else:\n",
    "        result=np.nan\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st=time()\n",
    "list_most_relevant_doc=[] #[{\"question_id\": int, \"similar\": [id_in_train_clean]}]\n",
    "for idx in range(len(train_ques_list)): \n",
    "    question_token_list=train_ques_list[idx][\"intent_tokens\"]\n",
    "    question_id=train_ques_list[idx][\"question_id\"]\n",
    "    \n",
    "    most_relevant_doc=get_most_relevant_document(question_token_list, trained_ft_vectors, document_embeddings)\n",
    "    #if question_id in list_most_relevant_doc:\n",
    "        # this exist\n",
    "    list_most_relevant_doc.append({\"question_id\": question_id, \"similar\": most_relevant_doc})\n",
    "print(\"Run time: {} s\".format(time()-st)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_most_relevant_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 cosine similarity are high. <br>\n",
    "Are the real match in the top 10 code snippet?<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Cosine Similarity between Question Title and Accepted Answer=> should be high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean = json.load(open(test_clean_output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x123638198>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_ft_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_embedding(token_list, word_embedding, vocab_size):\n",
    "    vec_ques=np.zeros((1, vocab_size))\n",
    "    token_count=0\n",
    "    has_token_in_embedding=False\n",
    "    for token in token_list:\n",
    "        if token in word_embedding:\n",
    "            has_token_in_embedding=True\n",
    "            vec_ques+=word_embedding[token]\n",
    "            token_count+=1\n",
    "    \n",
    "    if has_token_in_embedding:\n",
    "        return vec_ques/token_count\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time: 0.22294878959655762 s\n"
     ]
    }
   ],
   "source": [
    "st=time()\n",
    "result=[] #[{\"question_id\": int, \"similar\": [id_in_train_clean]}]\n",
    "for idx in range(len(test_clean)): \n",
    "    question_token_list=test_clean[idx][\"intent_tokens\"]\n",
    "    question_id=test_clean[idx][\"question_id\"]\n",
    "    snippet_token_list=test_clean[idx][\"snippet_tokens\"]\n",
    "    \n",
    "    ques_embed=get_doc_embedding(question_token_list, trained_ft_vectors, vocab_size)\n",
    "    code_embed=get_doc_embedding(snippet_token_list, trained_ft_vectors, vocab_size)\n",
    "    \n",
    "    if ques_embed.all()!=None and code_embed.all()!=None:\n",
    "        test_clean[idx][\"similarity\"]=cosine_similarity(ques_embed, code_embed)[0][0]\n",
    "        \n",
    "    else:\n",
    "        print(\"question_id {} has no token in wm.\".format(question_id))\n",
    "\n",
    "print(\"Run time: {} s\".format(time()-st)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization\n",
    "vis_data=[]\n",
    "for i, example in enumerate(test_clean):\n",
    "    vis_data.append(example[\"similarity\"])\n",
    "    \n",
    "sns.distplot(vis_data, axlabel=\"Cosine Similarity\", norm_hist=True)\n",
    "plt.ylabel(\"Count\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 if i>0.95 else 0 for i in vis_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2379"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2101723413198823"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "500/2379"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
